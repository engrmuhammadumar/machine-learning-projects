{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce GTX 1050 (UUID: GPU-6889ae7e-dcba-4438-63a4-ffb25e6a746c)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import create_tensorboard_callback, plot_loss_curves, compare_historys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "datasets_list = tfds.list_builders()\n",
    "print(\"food101\" in datasets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\Muhammad Umar\\tensorflow_datasets\\food101\\2.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1469c339b93c4afa856910e09edae08d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af6857490d84d7598d1be53e18ab6bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40af9ba4523845a5981fca51e6c8178e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(train_data, test_data), ds_info = tfds.load(name = \"food101\",\n",
    "                                             split = [\"train\", \"validation\"],\n",
    "                                             shuffle_files = True,\n",
    "                                             as_supervised=True,\n",
    "                                             with_info= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_one_sample \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "train_one_sample = train_data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features of Food101 TFDS\n",
    "ds_info.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class names\n",
    "class_names = ds_info.features[\"label\"].names\n",
    "class_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output info about our training sample\n",
    "for image, label in train_one_sample:\n",
    "  print(f\"\"\"\n",
    "  Image shape: {image.shape}\n",
    "  Image dtype: {image.dtype}\n",
    "  Target class from Food101 (tensor form): {label}\n",
    "  Class name (str form): {class_names[label.numpy()]}\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does an image tensor from TFDS's Food101 look like?\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the min and max values?\n",
    "tf.reduce_min(image), tf.reduce_max(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot an image tensor\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image)\n",
    "plt.title(class_names[label.numpy()]) # add title to image by indexing on class_names list\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function for preprocessing images\n",
    "def preprocess_img(image, label, img_shape=224):\n",
    "    \"\"\"\n",
    "    Converts image datatype from 'uint8' -> 'float32' and reshapes image to\n",
    "    [img_shape, img_shape, color_channels]\n",
    "    \"\"\"\n",
    "    image = tf.image.resize(image, [img_shape, img_shape]) # reshape to img_shape\n",
    "    return tf.cast(image, tf.float32), label # return (float32_image, label) tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess a single sample image and check the outputs\n",
    "preprocessed_img = preprocess_img(image, label)[0]\n",
    "print(f\"Image before preprocessing:\\n {image[:2]}...,\\nShape: {image.shape},\\nDatatype: {image.dtype}\\n\")\n",
    "print(f\"Image after preprocessing:\\n {preprocessed_img[:2]}...,\\nShape: {preprocessed_img.shape},\\nDatatype: {preprocessed_img.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can still plot our preprocessed image as long as we \n",
    "# divide by 255 (for matplotlib capatibility)\n",
    "plt.imshow(preprocessed_img/255.)\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map preprocessing function to training data (and paralellize)\n",
    "train_data = train_data.map(map_func=preprocess_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# Shuffle train_data and turn it into batches and prefetch it (load it faster)\n",
    "train_data = train_data.shuffle(buffer_size=1000).batch(batch_size=32).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Map prepreprocessing function to test data\n",
    "test_data = test_data.map(preprocess_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# Turn test data into batches (don't need to shuffle)\n",
    "test_data = test_data.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorBoard callback (already have \"create_tensorboard_callback()\" from a previous notebook)\n",
    "from helper_functions import create_tensorboard_callback\n",
    "\n",
    "# Create ModelCheckpoint callback to save model's progress\n",
    "checkpoint_path = \"model_checkpoints/cp.ckpt\" # saving weights requires \".ckpt\" extension\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                                      monitor=\"val_accuracy\", # save the model weights with best validation accuracy\n",
    "                                                      save_best_only=True, # only save the best weights\n",
    "                                                      save_weights_only=True, # only save model weights (not whole model)\n",
    "                                                      verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Turn on mixed precision training\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy(policy=\"mixed_float16\") # set global policy to mixed precision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_precision.global_policy() # should output \"mixed_float16\" (if your GPU is compatible with mixed precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# Create base model\n",
    "input_shape = (224, 224, 3)\n",
    "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
    "base_model.trainable = False # freeze base model layers\n",
    "\n",
    "# Create Functional model \n",
    "inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n",
    "# Note: EfficientNetBX models have rescaling built-in but if your model didn't you could have a layer like below\n",
    "# x = layers.Rescaling(1./255)(x)\n",
    "x = base_model(inputs, training=False) # set base_model to inference mode only\n",
    "x = layers.GlobalAveragePooling2D(name=\"pooling_layer\")(x)\n",
    "x = layers.Dense(len(class_names))(x) # want one output neuron per class \n",
    "# Separate activation of output layer so we can output float32 activations\n",
    "outputs = layers.Activation(\"softmax\", dtype=tf.float32, name=\"softmax_float32\")(x) \n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", # Use sparse_categorical_crossentropy when labels are *not* one-hot\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dtype_policy attributes of layers in our model\n",
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy) # Check the dtype policy of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the layers in the base model and see what dtype policy they're using\n",
    "for layer in model.layers[1].layers[:20]: # only check the first 20 layers to save output space\n",
    "    print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off all warnings except for errors\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Fit the model with callbacks\n",
    "history_101_food_classes_feature_extract = model.fit(train_data, \n",
    "                                                     epochs=3,\n",
    "                                                     steps_per_epoch=len(train_data),\n",
    "                                                     validation_data=test_data,\n",
    "                                                     validation_steps=int(0.15 * len(test_data)),\n",
    "                                                     callbacks=[create_tensorboard_callback(\"training_logs\", \n",
    "                                                                                            \"efficientnetb0_101_classes_all_data_feature_extract\"),\n",
    "                                                                model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model (unsaved version) on whole test dataset\n",
    "results_feature_extract_model = model.evaluate(test_data)\n",
    "results_feature_extract_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a function to recreate the original model\n",
    "def create_model():\n",
    "  # Create base model\n",
    "  input_shape = (224, 224, 3)\n",
    "  base_model = tf.keras.applications.efficientnet.EfficientNetB0(include_top=False)\n",
    "  base_model.trainable = False # freeze base model layers\n",
    "\n",
    "  # Create Functional model \n",
    "  inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n",
    "  # Note: EfficientNetBX models have rescaling built-in but if your model didn't you could have a layer like below\n",
    "  # x = layers.Rescaling(1./255)(x)\n",
    "  x = base_model(inputs, training=False) # set base_model to inference mode only\n",
    "  x = layers.GlobalAveragePooling2D(name=\"pooling_layer\")(x)\n",
    "  x = layers.Dense(len(class_names))(x) # want one output neuron per class \n",
    "  # Separate activation of output layer so we can output float32 activations\n",
    "  outputs = layers.Activation(\"softmax\", dtype=tf.float32, name=\"softmax_float32\")(x) \n",
    "  model = tf.keras.Model(inputs, outputs)\n",
    "  \n",
    "  return model\n",
    "\n",
    "# 2. Create and compile a new version of the original model (new weights)\n",
    "created_model = create_model()\n",
    "created_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                      optimizer=tf.keras.optimizers.Adam(),\n",
    "                      metrics=[\"accuracy\"])\n",
    "\n",
    "# 3. Load the saved weights\n",
    "created_model.load_weights(checkpoint_path)\n",
    "\n",
    "# 4. Evaluate the model with loaded weights\n",
    "results_created_model_with_loaded_weights = created_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Loaded checkpoint weights should return very similar results to checkpoint weights prior to saving\n",
    "import numpy as np\n",
    "assert np.isclose(results_feature_extract_model, results_created_model_with_loaded_weights).all(), \"Loaded weights results are not close to original model.\"  # check if all elements in array are close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the layers in the base model and see what dtype policy they're using\n",
    "for layer in created_model.layers[1].layers[:20]: # check only the first 20 layers to save printing space\n",
    "    print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model locally (if you're using Google Colab, your saved model will Colab instance terminates)\n",
    "save_dir = \"07_efficientnetb0_feature_extract_model_mixed_precision\"\n",
    "model.save(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model previously saved above\n",
    "loaded_saved_model = tf.keras.models.load_model(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the layers in the base model and see what dtype policy they're using\n",
    "for layer in loaded_saved_model.layers[1].layers[:20]: # check only the first 20 layers to save output space\n",
    "    print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check loaded model performance (this should be the same as results_feature_extract_model)\n",
    "results_loaded_saved_model = loaded_saved_model.evaluate(test_data)\n",
    "results_loaded_saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
